---
draft: true
title: 'Stretching my Legs in the Analytical Data Ecosystem in 2022'
date: "2022-09-02T10:42:30Z"
thumbnail: "/images/2022/09/IMG_7557.jpeg"
image: "/images/2022/09/RemoteMediaFile_6554199_0_2022_07_15_21_21_20.jpeg"
credit: "https://twitter.com/rmoff/"
categories:
- Data Engineering
---

:source-highlighter: rouge
:icons: font
:rouge-css: style
:rouge-style: github

<!--more-->

For the past 5.5 years I've been head-down in the exciting area of stream processing and events, and I realised recently that the world of data and analytics that I worked in up to 2017 which was changing significantly back then (Big Data, y'all!) has evolved and, dare I say it, matured somewhat - and I've not necessarily kept up with it. In this post you can follow along as I start to reacquaint myself with where it's got to these days.

## Background

Twenty years ago (😱TWENTY😱) I took my first job from university using my 🎼 Music degree to…build data warehouses. I was lucky to get a place on a graduate scheme at a well-established retailer with an excellent training programme. I got taught COBOL, DB2, TSO, and all that fun stuff. I even remember my TSO handle - TSO954. Weird huh. From there I continued in the data space, via a stint as a DBA on production OLTP systems, and into more data warehousing and analytics with Oracle in 2010. 

From the mid 2010s I became aware of the Big Data ecosystem emerging, primarily around Apache Hadoop and Cloudera. All the things I'd been used to doing with analytical data suddenly became really difficult. Query some data? Write Java code. Update some data? Haha nope. Use SQL? hah, welcome to this alpha release that probably doesn't work. And BTW, if you didn't know it before, now you truly know the meaning of JAR hell. 

Snark aside, I spent some time https://www.rittmanmead.com/blog/2016/12/etl-offload-with-spark-and-amazon-emr-part-5/[looking at some of the tools] and building out examples of its use in 2016, before moving jobs into my current role at Confluent. 

At Confluent I've been working with Apache Kafka and diving deep into the world of stream processing, including building streaming data pipelines. When I took on the role of chair of the https://www.confluent.io/en-gb/blog/introducing-current-2022-program-committee/[program committee for Current 22] part of the remit was to help source and build a program that included elements across the broader data landscape than Kafka alone. In doing this, I realised quite how much had changed in recent years, and gave me an itch to try and make some sense of it. 

Herewith that itch being scratched… 

+++
<div class="tenor-gif-embed" data-postid="15016547" data-share-method="host" data-aspect-ratio="1" data-width="100%"><a href="https://tenor.com/view/happy-scratch-head-cat-gif-15016547">Happy Scratch Head GIF</a>from <a href="https://tenor.com/search/happy-gifs">Happy GIFs</a></div> <script type="text/javascript" async src="https://tenor.com/embed.js"></script>
+++

## How It Started

It's hard to write a piece like this without invoking https://www.youtube.com/watch?v=ue7wM0QC5LE[The Four Yorkshiremen] at some point, so I'll get it out of the way now. But +++<del>+++back in my day+++</del>+++ my starting point for what is nowadays called `<Data|Analytics> Engineering` is what pretty much everyone—bar the real cool kids in silicon valley—was doing back in the early 2010s (and the previous ~decades before that): 

* Find a source system - probably just one, or maybe two. Almost certainly a mainframe, Oracle, flat files. 
* ETL/ELT the data into a target Data Warehouse or Data Mart, probably with a star or snowflake schema
* Build dashboards on the data

Extracts were usually once a day. Operational Data Stores were becoming a bit trendy then along with 'micro-batch' extracts which meant data coming maybe more frequently, e.g. 5-10 minutes

Tools were the traditional ETL lot (Informatica, Data Stage, etc), with Oracle's Data Integrator (neé Sunopsis) bringing the concept of ELT. Instead of buying servers (e.g. Informatica) to run the Transform, you took advantage of the target database's ability to crunch data by loading the data into the database in one set of tables, transform it there, and load it into another set of tables.

Data modeling was an important part, as was the separation of logical and physical at both the ELT stage (ODI) and reporting (e.g. OBIEE's semantic layer). 

Source control was something that the hipsters did. Emailing around files and using CODE_FILE_V01, CODE_FILE_V02, CODE_FILE_PROD_RELESE!_V3 was seen as perfectly acceptable and sufficient. Even if you wanted to, it was often https://www.rittmanmead.com/blog/2015/01/concurrent-rpd-development-in-obiee/[difficult]. 

Data Warehouses tended to live on dedicated platforms including Teradata, Netezza, as well as the big RDBMSs such as Oracle, DB2, and SQL Server. 

Scaling was mostly vertical (buy a bigger box, put in more CPUs). Oracle's Exadata was an engineered system and launched with the promise of magical performance improvements with a combination of hardware and software fairy dust to make everything go quick. 

The idea of building around open source software was not a commonplace idea for most companies of any size, and Cloud was in the early phases of the hype cycle. 

Analytics work was centralised around the BI (Business Information) or MIS (Management Information Systems) teams, or going back a bit further the DSS (Decision Support System) team. Source systems owners would grudgingly allow an extract from their systems, with domain model specifics often left to the analytics team to figure out. 

## How It's Going

_First, a HUGE caveat: whereas the above is written based on my experience as a "practitioner" (I was actually doing and building this stuff), what comes next is my _perception_ from conversations, my Twitter bubble, etc. I would be deeply pleased to be corrected on any skew in what I write._

Where to start? 

The use of data has changed. The technology to do so has changed. The general technical competence and willingness to interact with IT systems has changed. 

Data is no longer used just to print out on a report on a Monday morning. Data is used throughout companies to inform and automate processes, as described by Jay Kreps in his article https://www.confluent.io/blog/every-company-is-becoming-software/[Every Company is Becoming +++<del>+++a+++</del>+++ Software +++<del>+++Company+++</del>+++]. 

Handling of data is done by many teams, not just one. In the new world of the Data Mesh even source system owners get in on the game and publish their data as a data product. Outside of IT, other departments in a company are staffed by a new generation of "technology native" employees - those who grew up with the internet and computers as just as much a part of their worlds as television and the telephone. Rather than commissioning a long project from the IT department they're as likely to self-serve their requirements, either building something themselves or using a credit card to sign up for one of the thousands of SaaS platforms. 

Hardware is not limited to what you can procure and provision in your data center through a regimented and standardised central process, but whatever you can imagine (and afford) in the Cloud. Laptops themselves are so powerful that much processing can just be done locally. And the fact that it's laptops is notable - now anyone can work anywhere. The desktop machine in the office is becoming an anachronistic idea except for specialised roles such as video processing. 

The world of software has been blown apart, driven in my opinion by the internet, open source, and Cloud. Companies no longer choose between paying IBM for a mainframe license or Microsoft for a Windows licence, but whether to pay at all. Linux went from being a niche geek interest to the foundation on which a huge number of critical systems run. Oracle is still a dominant player in the RDBMS world but you're no longer an oddity if you propose to use Postgres instead. The world of software is at our disposal and just a download link away. With Docker you can spin up and try a dozen data stores in a day and pick the one that suits you best. 

And speaking of a dozen data stores, nowadays there are stores specialised for every purpose. NoSQL, oldSQL, someSQL, newSQL and everywhere in between. Graph, relational, and document. AWS in particular has leant into this, mirroring what's available to run yourself in their plethora of SaaS offerings in the data space. 

For the rest of the article I'm going to focus on the software side of things. 

## Back to Basics

Both https://seattledataguy.substack.com/[SeattleDataGuy] in his article https://seattledataguy.substack.com/p/the-baseline-datastack-going-beyond[The Baseline Data Stack] and https://www.linkedin.com/in/jamesdensmore/[James Densmore] in his book https://www.oreilly.com/library/view/data-pipelines-pocket/9781492087823/[Data Pipelines Pocket Reference] describe the basic approach to moving data into a place from which it can be analysed. Build a pipeline to do a batch extract of data from the source system into a target store from which it can be worked on. No streaming, no fancy tooling - just good ole' ETL. 

But what is the data store? How do you extract, load, and transform the data? 

It seems that nowadays you're as likely to be writing code (probably in Python, but maybe Java) as opening up a GUI to build your pipeline. You might be loading data into an RDBMS from which to analyse it but quite often it'll be an object store such as S3, or maybe a cloud datawarehouse like Snowflake. 

A plethora of tools have sprung up to orchestrate these kinds of pipelines, with https://seattledataguy.substack.com/p/should-you-use-apache-airflow[Airflow] a common one. 

## Storing and Accessing Your Data pt 1: 🔥 Burn It All Down 🔥

In the beginning was the word, and the word was an expensive relational datawarehouse that wasn't flexible or scalable enough for the cool kids in Silicon Valley. 

Then came Hadoop and scalability was won, at the vast cost of usability. You literally had to write your own Java code to move data around and transform it. You needed to serialise and deserialise the data yourself, and could store whatever you wanted - it didn't have to be structured. This was sold as a benefit -- "Schema on Read" they said, "It'll be a good idea", they said. _Oh bugger, where's my schema, they said when they came to use it._

Through the virtues of open source a fantastic ecosystem grew and such wonderfully named projects as Sqoop, Oozie, and Pig and Flume emerged. Hive brought with it the familiar and comforting bosom of SQL and table structures but with limited functionality (no `DELETE` or `UPDATE` at first) and performance. 

Over the years things improved, with Spark replacing MapReduce and enabling a generation of Python coders to get into the big data lark too, along with https://spark.apache.org/sql/[SQL].

Amongst all of this pioneering work and technology was the assumption that the resting (and sometimes serving) place for analytical data was HDFS. Other stores like HBase existed for special purposes, but the general we've-got-a-bunch-of-data-in-this-org-and-need-to-collect-it destination was HDFS. Because "general dumping ground" wasn't sexy enough for the marketing folk it became sold as a "Data Lake" with all the associated puns and awful cliches (fishing for data, data swamp, etc etc etc). 

The general pitch around the data lake was to collect all the data, structured and unstructured (or structured that you've made unstructured by chucking away its schema when you loaded it) and then +++<del>+++wait for the data lake fairy to conjure magical value out of the pile of data you've dumped there+++</del>+++ make the raw data available for teams in the company to process and use for their particular purposes. 

All of this was built around the closely-intertwined Hadoop platform, whether self-managed on-premises or with a cloud provider such as AWS. In parallel you had lots of other development enabling similar patterns, such as Athena (built on the open-source Presto) to query data held in S3. 

## Storing and Accessing Your Data pt 2: …and Then Rebuild It 🏗️

Coming back to this after my attention being elsewhere for a few years means that I have the slightly uninformed but helpfully simplistic view of things. What the relational datawarehouses used to do (bar scale, arguably), we are now approaching something roughly like parity again with a stack of tools that have stabilised and matured in large, with a layer on top that's still being figured out. 

Underpinning it all is the idea of separation of storage and compute. This is important for two vital reasons: 

* It's a lot cheaper to store data and pay for compute when you want to use it, instead of the coupled approach (per RDBMS like Oracle) where you have a server with disks and CPUs and you're paying for the CPUs whether they're doing anything or not, and when you need more storage and have filled the disk bays you need to buy (and license, hi Oracle) another server (with CPUs etc)
* If your data is held in an open format on a storage platform that has open APIs then multiple tools can use it as they want to. Contrast this to putting your data in SQL Server (not to pick on Oracle all the time), and any tool that wants to use it has to do so through SQL Server. If a new tool comes along that does particular processing really really well and your data is sat in an RDBMS then you have to migrate that data off the RDBMS first, or query it in place. 

<< point about cloud native and how it's even more important to have open formats>> 

This is where a lot of the innovation seems to be happening at the moment, with table formats being built and developed to work with data on these open storage platforms (HDFS, S3, etc) held in file formats suitable to the task at hand (such as https://parquet.apache.org/[Apache Parquet] which is column based and thus great for analytics). Table formats include: 

* Apache Hudi
* Apache Iceberg
* Delta Lake

https://www.dremio.com/subsurface/subsurface-meetup-comparison-of-data-lakehouse-table-formats/

All of them enable the things that we'd have taken for granted a decade ago including rich metadata, `UPDATE`s, `DELETE`s, and ACID compliance. 

## Job Titles

Back in the day, you were often a programmer, a datawarehouse specialist, a BI analyst, and all and many titles in between. 

Nowadays you have people who get the actual value out of the data that pays for all of this to happen, and they might still be called Analysts of one flavour or another but more often Data Scientists. This overlaps and bleeds into the ML world too. 

For a few years the people who got the data for the analysts to work with were https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603[*Data Engineers*] (modelling the Software Engineers that "programmers" of old had become). It seems to me that this label has split further, with Data Engineering being the discipline of getting the data out of the source, building the pipelines to get it into some kind of staging area (e.g. data lake). From here the https://benn.substack.com/p/why-do-people-want-to-be-analytics[*Analytics Engineers*] take over, cleansing and perhaps restructuring it into a form and schema that is accessible and performant for the required use.




## Resources

https://raindrop.io/rmoff/data-engineering-23335742

https://www.getdbt.com/blog/future-of-the-modern-data-stack/[Tristan Handy - The Modern Data Stack: Past, Present, and Future]
https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603[Maxime Beauchemin - The Rise of the Data Engineer]

Newsletters e.g. Benn Stancil, David J

