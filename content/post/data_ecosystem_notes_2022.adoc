---
draft: true
title: 'Stretching my Legs in the Analytical Data Ecosystem in 2022'
date: "2022-08-30T16:42:30Z"
thumbnail: "/images/2022/07/IMG_7557.jpeg"
image: "/images/2022/07/RemoteMediaFile_6554199_0_2022_07_15_21_21_20.jpeg"
credit: "https://twitter.com/rmoff/"
categories:
- Data Engineering
---

:source-highlighter: rouge
:icons: font
:rouge-css: style
:rouge-style: github

<!--more-->

For the past 5.5 years I've been head-down in the exciting area of stream processing and events, and I realised recently that the world of data and analytics that I worked in up to 2017 which was evolving back then has evolved and, dare I say it, matured since then - and I've not necessarily kept up with it. In this post you can follow along as I reacquaint myself with where it's got to these days.

## Background

Twenty years ago (ðŸ˜±TWENTYðŸ˜±) I took my first job from university using my ðŸŽ¼ Music degree toâ€¦build data warehouses. I was lucky to get a place on a graduate scheme at a well-established retailer with an excellent training programme. I got taught COBOL, DB2, TSO, and all that fun stuff. I even remember my TSO handle - TSO954. Weird huh. From there I continued in the data space, via a stint as a DBA on production OLTP systems, and into more data warehousing and analytics with Oracle in 2010. 

From the mid 2010s I became aware of the Big Data ecosystem emerging, primarily around Apache Hadoop and Cloudera. All the things I'd been used to doing with analytical data suddenly became really difficult. Query some data? Write Java code. Update some data? Haha nope. Use SQL? hah, welcome to this alpha release that probably doesn't work. And BTW, if you didn't know it before, now you truly know the meaning of JAR hell. 

Snark aside, I spent some time https://www.rittmanmead.com/blog/2016/12/etl-offload-with-spark-and-amazon-emr-part-5/[looking at some of the tools] and building out examples of its use in 2016, before moving jobs into my current role at Confluent, working with Apache Kafka. Here I dived deep into the world of stream processing, including building streaming data pipelines. 

When I took on the role of chair of the program committee for Current 22 part of the remit was to help source and build a program that included elements across the broader data landscape than Kafka alone. In doing this, I realised quite how much had changed in recent years, and gave me an itch to try and make some sense of it. 

Herewith that itch being scratchedâ€¦ 

## How It Started

It's hard to write a piece like this without invoking https://www.youtube.com/watch?v=ue7wM0QC5LE[The Four Yorkshiremen] at some point, so I'll get it out of the way now. But ~~~back in my day~~~ my starting point for what is nowadays called <Data|Analytics> Engineering is what pretty much everyoneâ€”bar the real cool kids in silicon valleyâ€”was doing back in the early 2010s (and the previous ~decades before that): 

* Find a source system - probably just one, or maybe two. Almost certainly a mainframe, Oracle, flat files. 
* ETL/ELT the data into a target Data Warehouse or Data Mart, probably with a star or snowflake schema
* Build dashboards on the data

Extracts were usually once a day. Operational Data Stores were becoming a bit trendy then along with 'micro-batch' extracts which meant data coming maybe more frequently, e.g. 5-10 minutes

Tools were the traditional ETL lot (Informatica, Data Stage, etc), with Oracle's Data Integrator (neÃ© Sunopsis) bringing the concept of ELT. 

Data modeling was an important part, as was the separation of logical and physical at both the ELT stage (ODI) and reporting (e.g. OBIEE's semantic layer). 



## How It's Going


## Resources

https://raindrop.io/rmoff/data-engineering-23335742

Newsletters e.g. Benn Stancil, David J