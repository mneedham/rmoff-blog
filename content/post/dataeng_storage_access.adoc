---
draft: true
title: 'Dataeng_storage_access'
date: "2022-09-07T17:07:04Z"
image: "/images/2022/09/"
thumbnail: "/images/2022/09/"
credit: "https://twitter.com/rmoff/"
categories:
- Uncategorized
---

:source-highlighter: rouge
:icons: font
:rouge-css: style
:rouge-style: github

<!--more-->


## Storing and Accessing Your Data pt 1: üî• Burn It All Down üî•

In the beginning was the word, and the word was an expensive relational datawarehouse that wasn't flexible or scalable enough for the cool kids in Silicon Valley. 

Then came Hadoop and scalability was won, at the vast cost of usability. You literally had to write your own Java code to move data around and transform it. You needed to serialise and deserialise the data yourself, and could store whatever you wanted - it didn't have to be structured. This was sold as a benefit -- "Schema on Read" they said, "It'll be a good idea", they said. _Oh bugger, where's my schema, they said when they came to use it._

Through the virtues of open source a fantastic ecosystem grew and such wonderfully named projects as Sqoop, Oozie, and Pig and Flume emerged. Hive brought with it the familiar and comforting bosom of SQL and table structures but with limited functionality (including no `DELETE` or `UPDATE` at first) and performance. 

Over the years things improved, with Spark replacing MapReduce and enabling a generation of Python coders to get into the big data lark too, along with https://spark.apache.org/sql/[SQL].

Amongst all of this pioneering work and technology was the assumption that the resting (and sometimes serving) place for analytical data was HDFS. Other stores like HBase existed for special purposes, but the general we've-got-a-bunch-of-data-in-this-org-and-need-to-collect-it destination was HDFS. Because "general dumping ground" wasn't sexy enough for the marketing folk it became sold as a "Data Lake" with all the associated puns and awful cliches (fishing for data, data swamp, etc etc etc). 

The general pitch around the data lake was to collect all the data, structured and unstructured (or structured that you've made unstructured by chucking away its schema when you loaded it) and then +++<del>+++wait for the data lake fairy to conjure magical value out of the pile of data you've dumped there+++</del>+++ make the raw data available for teams in the company to process and use for their particular purposes. 

All of this was built around the closely-intertwined Hadoop platform, whether self-managed on-premises or with a cloud provider such as AWS. In parallel you had lots of other development enabling similar patterns, such as Athena (built on the open-source Presto) to query data held in S3. 

## Storing and Accessing Your Data pt 2: ‚Ä¶and Then Rebuild It üèóÔ∏è


Coming back to this after my attention being elsewhere for a few years means that I have the slightly uninformed but helpfully simplistic view of things. What the relational datawarehouses used to do (bar scale, arguably), we are now approaching something roughly like parity again with a stack of tools that have stabilised and matured in large, with a layer on top that's still being figured out. 

Underpinning it all is the idea of separation of storage and compute. This is important for two vital reasons: 

* It's a lot cheaper to store data and pay for compute when you want to use it, instead of the coupled approach (per RDBMS like Oracle) where you have a server with disks and CPUs and you're paying for the CPUs whether they're doing anything or not, and when you need more storage and have filled the disk bays you need to buy (and license, hi Oracle) another server (with CPUs etc). 
* If your data is held in an open format on a storage platform that has open APIs then multiple tools can use it as they want to. Contrast this to putting your data in SQL Server (not to pick on Oracle all the time), and any tool that wants to use it has to do so through SQL Server. If a new tool comes along that does particular processing really really well and your data is sat in an RDBMS then you have to migrate that data off the RDBMS first, or query it in place. 

Hive gave us this separation, right? Well, it did, but with a https://youtu.be/nWwQMlrjhy0?t=734[long list of problems and limitations].

### The Lake House

The open format point could also be reframed as the "vendor lockin bogey man" pitch, but rather than that I would say that it's more about creating a platform that enables you to hedge your bets as to the best tools that will be using the data. 

<< point about cloud native and how it's even more important to have open formats>> 

Lakehouse paper 2021 https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf
https://twitter.com/gwenshap/status/1565771009902256129
https://www.linkedin.com/feed/update/urn:li:activity:6971514189613834240/

This is where a lot of the innovation seems to be happening at the moment, with table formats being built and developed to work with data on these open storage platforms (HDFS, S3, etc) held in file formats suitable to the task at hand (such as https://parquet.apache.org/[Apache Parquet] which is column based and thus great for analytics). Table formats include: 

* Apache Hudi
* Apache Iceberg
* Delta Lake

All of them enable the things that we'd have taken for granted a decade ago including rich metadata, transactional access, `UPDATE`s, `DELETE`s, and ACID compliance, along with apparently performant access to the underlying raw data files. 

There are some good explanation and comparison posts covering the three formats: 
* https://dacort.dev/posts/modern-data-lake-storage-layers/[An Introduction to Modern Data Lake Storage Layers] - https://twitter.com/dacort[Damon Cortesi] (AWS)
* Comparison of Data Lake Table Formats https://www.dremio.com/subsurface/comparison-of-data-lake-table-formats-iceberg-hudi-and-delta-lake/[blog] / https://www.dremio.com/subsurface/subsurface-meetup-comparison-of-data-lakehouse-table-formats/[video] - https://twitter.com/AMdatalakehouse[Alex Merced] (Dremio)
* https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison[Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison] - https://www.linkedin.com/in/lakehouse/[Kyle Weller] (Onehouse)

Run your own, or use used hosted versions including

* https://www.onehouse.ai/[Onehouse] (Apache Hudi)
* https://tabular.io/[Tabular] (Apache Iceberg)
* https://www.databricks.com/[Databricks] (Delta Lake) 
* GCP's (https://cloud.google.com/blog/products/data-analytics/unify-data-lakes-and-warehouses-with-biglake-now-generally-available[BigLake])

_Azure have a close partnership with Databricks, so the only major cloud provider missing from this list is AWS. They have https://aws.amazon.com/lake-formation/[Lake Formation] and https://docs.aws.amazon.com/lake-formation/latest/dg/governed-tables.html[Governed Tables] which looks similar on the surface but I've not dug into in detail (and Governed Tables aren't even mentioned on AWS' https://aws.amazon.com/blogs/big-data/build-a-lake-house-architecture-on-aws/[Build a Lake House Architecture on AWS] blog)._

Snowflake recently added support for https://www.snowflake.com/blog/iceberg-tables-powering-open-standards-with-snowflake-innovations/[Iceberg tables] (complementing the existing https://docs.snowflake.com/en/user-guide/tables-external-intro.html#delta-lake-support[support for Delta Lake external tables]), and are https://www.snowflake.com/blog/5-reasons-apache-iceberg/[backing Iceberg] presumably in part to try and hamper Databricks' Delta Lake (c.f. "_Iceberg includes features that are *paid in other table formats*_", "_The Iceberg project is *well-run* open source_" etc). 

https://www.dremio.com/[Dremio] are also in this space as one of the companies working on https://arrow.apache.org/[Apache Arrow] and providing a fast query engine built on it called Dremio Sonar. I've yet to get my head around their offering, but it looks like on-premises platform as well as hosted, with support for Apache Iceberg and Delta Lake. They've got a rich set of resources in their https://www.dremio.com/subsurface/[Subsurface] resource area.

https://www.youtube.com/watch?v=3znQs0MzZ-k[Data Lakehouse and Data Mesh‚ÄîTwo Sides of the Same Coin]
Zalando have 10 petabytes of data in central data lake, and same again across other buckets across the organisation.

Spark on Databricks
Trino + Superset 
Redshift for local datawarehouses
SageMaker

https://www.youtube.com/watch?v=3znQs0MzZ-k&t=1818s[Data Mesh is a paradigm - Lakehouse is a platform]
