---
draft: true
title: 'Dataeng_storage_access'
date: "2022-09-07T17:07:04Z"
image: "/images/2022/09/"
thumbnail: "/images/2022/09/"
credit: "https://twitter.com/rmoff/"
categories:
- Uncategorized
---

:source-highlighter: rouge
:icons: font
:rouge-css: style
:rouge-style: github

<!--more-->


## Storing and Accessing Your Data pt 1: üî• Burn It All Down üî•

In the beginning was the word, and the word was an expensive relational datawarehouse that wasn't flexible or scalable enough for the cool kids in Silicon Valley. 

Then came Hadoop and scalability was won, at the vast cost of usability. You literally had to write your own Java code to move data around and transform it. You needed to serialise and deserialise the data yourself, and could store whatever you wanted - it didn't have to be structured. This was sold as a benefit -- "Schema on Read" they said, "It'll be a good idea", they said. _Oh bugger, where's my schema, they said when they came to use it._

Through the virtues of open source a fantastic ecosystem grew and such wonderfully named projects as Sqoop, Oozie, and Pig and Flume emerged. Hive brought with it the familiar and comforting bosom of SQL and table structures but with limited functionality (including no `DELETE` or `UPDATE` at first) and performance. 

Over the years things improved, with Spark replacing MapReduce and enabling a generation of Python coders to get into the big data lark too, along with https://spark.apache.org/sql/[SQL].

Amongst all of this pioneering work and technology was the assumption that the resting (and sometimes serving) place for analytical data was HDFS. Other stores like HBase existed for special purposes, but the general we've-got-a-bunch-of-data-in-this-org-and-need-to-collect-it destination was HDFS. Because "general dumping ground" wasn't sexy enough for the marketing folk it became sold as a "Data Lake" with all the associated puns and awful cliches (fishing for data, data swamp, etc etc etc). 

The general pitch around the data lake was to collect all the data, structured and unstructured (or structured that you've made unstructured by chucking away its schema when you loaded it) and then +++<del>+++wait for the data lake fairy to conjure magical value out of the pile of data you've dumped there+++</del>+++ make the raw data available for teams in the company to process and use for their particular purposes. 

All of this was built around the closely-intertwined Hadoop platform, whether self-managed on-premises or with a cloud provider such as AWS. In parallel you had lots of other development enabling similar patterns, such as Athena (built on the open-source Presto) to query data held in S3. 

## Storing and Accessing Your Data pt 2: ‚Ä¶and Then Rebuild It üèóÔ∏è


Coming back to this after my attention being elsewhere for a few years means that I have the slightly uninformed but helpfully simplistic view of things. What the relational datawarehouses used to do (bar scale, arguably), we are now approaching something roughly like parity again with a stack of tools that have stabilised and matured in large, with a layer on top that's still being figured out. 

Underpinning it all is the idea of separation of storage and compute. This is important for two vital reasons: 

* It's a lot cheaper to store data and pay for compute when you want to use it, instead of the coupled approach (per RDBMS like Oracle) where you have a server with disks and CPUs and you're paying for the CPUs whether they're doing anything or not, and when you need more storage and have filled the disk bays you need to buy (and license, hi Oracle) another server (with CPUs etc). 
* If your data is held in an open format on a storage platform that has open APIs then multiple tools can use it as they want to. Contrast this to putting your data in SQL Server (not to pick on Oracle all the time), and any tool that wants to use it has to do so through SQL Server. If a new tool comes along that does particular processing really really well and your data is sat in an RDBMS then you have to migrate that data off the RDBMS first, or query it in place. 

Hive gave us this separation, right? Well, it did, but with a https://youtu.be/nWwQMlrjhy0?t=734[long list of problems and limitations].

### The Lake House

The open format point could also be reframed as the "vendor lockin bogey man" pitch, but rather than that I would say that it's more about creating a platform that enables you to hedge your bets as to the best tools that will be using the data. 

<< point about cloud native and how it's even more important to have open formats>> 

Lakehouse paper 2021 https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf
https://twitter.com/gwenshap/status/1565771009902256129
https://www.linkedin.com/feed/update/urn:li:activity:6971514189613834240/

This is where a lot of the innovation seems to be happening at the moment, with table formats being built and developed to work with data on these open storage platforms (HDFS, S3, etc) held in file formats suitable to the task at hand (such as https://parquet.apache.org/[Apache Parquet] which is column based and thus great for analytics). Table formats include: 

* Apache Hudi
* Apache Iceberg
* Delta Lake

All of them enable the things that we'd have taken for granted a decade ago including rich metadata, transactional access, `UPDATE`s, `DELETE`s, and ACID compliance, along with performant access when querying the data. 

There are some good explanation and comparison posts covering the three formats: 
* https://dacort.dev/posts/modern-data-lake-storage-layers/[An Introduction to Modern Data Lake Storage Layers] - https://twitter.com/dacort[Damon Cortesi] (AWS)
* Comparison of Data Lake Table Formats https://www.dremio.com/subsurface/comparison-of-data-lake-table-formats-iceberg-hudi-and-delta-lake/[blog] / https://www.dremio.com/subsurface/subsurface-meetup-comparison-of-data-lakehouse-table-formats/[video] - https://twitter.com/AMdatalakehouse[Alex Merced] (Dremio)
* https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison[Apache Hudi vs Delta Lake vs Apache Iceberg - Lakehouse Feature Comparison] - https://www.linkedin.com/in/lakehouse/[Kyle Weller] (Onehouse)
* https://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/[Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared] - https://www.linkedin.com/in/paulsingman/[Paul Singman]

Run your own, or use used hosted versions including

* https://www.onehouse.ai/[Onehouse] (Apache Hudi)
* https://tabular.io/[Tabular] (Apache Iceberg)
* https://www.databricks.com/[Databricks] (Delta Lake) 
* GCP's (https://cloud.google.com/blog/products/data-analytics/unify-data-lakes-and-warehouses-with-biglake-now-generally-available[BigLake])

_Azure have a close partnership with Databricks, so the only major cloud provider missing from this list is AWS. They have https://aws.amazon.com/lake-formation/[Lake Formation] and https://docs.aws.amazon.com/lake-formation/latest/dg/governed-tables.html[Governed Tables] which looks similar on the surface but I've not dug into in detail (and Governed Tables aren't even mentioned on AWS' https://aws.amazon.com/blogs/big-data/build-a-lake-house-architecture-on-aws/[Build a Lake House Architecture on AWS] blog)._

Snowflake recently added support for https://www.snowflake.com/blog/iceberg-tables-powering-open-standards-with-snowflake-innovations/[Iceberg tables] (complementing the existing https://docs.snowflake.com/en/user-guide/tables-external-intro.html#delta-lake-support[support for Delta Lake external tables]), and are https://www.snowflake.com/blog/5-reasons-apache-iceberg/[backing Iceberg] presumably in part to try and hamper Databricks' Delta Lake (c.f. "_Iceberg includes features that are *paid in other table formats*_", "_The Iceberg project is *well-run* open source_" etc). 

https://www.dremio.com/[Dremio] are also in this space as one of the companies working on https://arrow.apache.org/[Apache Arrow] and providing a fast query engine built on it called Dremio Sonar. I've yet to get my head around their offering, but it looks like on-premises platform as well as hosted, with support for Apache Iceberg and Delta Lake. They've got a rich set of resources in their https://www.dremio.com/subsurface/[Subsurface] resource area.

https://www.youtube.com/watch?v=3znQs0MzZ-k[Data Lakehouse and Data Mesh‚ÄîTwo Sides of the Same Coin]
Zalando have 10 petabytes of data in central data lake, and same again across other buckets across the organisation.

Spark on Databricks
Trino + Superset 
Redshift for local datawarehouses
SageMaker

https://www.youtube.com/watch?v=3znQs0MzZ-k&t=1818s[Data Mesh is a paradigm - Lakehouse is a platform]

## LakeFS -- `git` For Data

https://twitter.com/rmoff/status/1567829714865102853[src]

Having https://www.youtube.com/watch?v=uixZ7NcGoeE[watched @gwenshap and @ozkatz100 talk about "git for data"] I would definitely say is a serious idea.
However to the point at the end of the video, RTFM‚Äîit took reading https://docs.lakefs.io/using_lakefs/data-devenv.html and some other pages subsequently to really grok the concept in practice.

Where I struggled at first with the git analogy alone was that data changes, and I couldn't see how branch/merge fitted into that outside of the idea of branching for throwaway testing alone. The https://www.youtube.com/watch?v=uixZ7NcGoeE&t=1401s[1PB accidental deletion example] was useful for illustrating the latter point for sure. 

But then reading https://docs.lakefs.io/understand/roadmap.html#improved-streaming-support-for-apache-kafka made me realise that I was thinking about the whole thing from a streaming PoV‚Äîwhen actually the idea of running a batch against a branch with a hook to validate and then merge is a freakin awesome idea

(As the roadmap issue notes, doing this for streaming data is conceptually possible but more complex to implement.) 

I'm also still trying to think through the implications of https://docs.lakefs.io/understand/model.html#merge[merging one branch into another] in which there are changes; can data really be treated the same as code in that sense, or could one end up with inconsistent data sets?

Lastly, having been reading up on table formats, I'd be interested to dig into quite how much LakeFS works already with them vs roadmap alone (the docs are not entirely consistent on this point)‚Äîbut with both in place it sounds like a fantastic place for data eng to be heading. 



https://lakefs.io/the-everything-bagel-ii-versioned-data-lake-tables-with-lakefs-and-trino/



Roll back deletes
Test new ETL with live data

Live *batch* workloads: 

> * run a job or a pipeline on a separate branch and commit,
> * record information such as the git hash of the code executed, the versions of frameworks used, and information about the data artifacts,
> * once the pipeline has completed successfully, commit, and attach the recorded information as metadata.
https://docs.lakefs.io/understand/roadmap.html#improved-streaming-support-for-apache-kafka[src] 

https://www.youtube.com/watch?v=uixZ7NcGoeE&t=1237s[How to commit an unbounded stream?]
this one I don't quite get. Suggests there's some kind of quality check that's applied before merging the branch. is this automated - CI/CD approach? Yes, using hooks just like in github world. See https://docs.lakefs.io/understand/roadmap.html#improved-streaming-support-for-apache-kafka. Aspirational pattern, not easily implemented currently. 

LakeFS works just fine with Hive, but per the https://docs.lakefs.io/understand/roadmap.html#table-format-support[roadmap] it sounds like there's not full support for modern table format (Hudi/Iceberg/Delta Lake) yet.